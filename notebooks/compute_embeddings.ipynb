{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "from protein_clip import get_model, embed_sequence, batch_embed_sequence\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"../data/uniprot_sprot.paruqet.gz\") as f:\n",
    "    df = pd.read_parquet(f)\n",
    "df\n",
    "del f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"sequence\"].apply(len) < 3_000].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESM2(\n",
       "  (embed_tokens): Embedding(33, 320, padding_idx=1)\n",
       "  (layers): ModuleList(\n",
       "    (0-5): 6 x TransformerLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (k_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (v_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (q_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (out_proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "        (rot_emb): RotaryEmbedding()\n",
       "      )\n",
       "      (self_attn_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "      (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "      (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "      (final_layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (contact_head): ContactPredictionHead(\n",
       "    (regression): Linear(in_features=120, out_features=1, bias=True)\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=320, out_features=320, bias=True)\n",
       "    (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want to run all the sequences through the esm model. Which size?\n",
    "layer = 6\n",
    "model, alphabet = get_model(layer)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accession</th>\n",
       "      <th>name</th>\n",
       "      <th>description</th>\n",
       "      <th>sequence</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q6GZX4</td>\n",
       "      <td>001R_FRG3G</td>\n",
       "      <td>Putative transcription factor 001R OS=Frog vir...</td>\n",
       "      <td>MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQV...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q6GZX3</td>\n",
       "      <td>002L_FRG3G</td>\n",
       "      <td>Uncharacterized protein 002L OS=Frog virus 3 (...</td>\n",
       "      <td>MSIIGATRLQNDKSDTYSAGPCYAGGCSAFTPRGTCGKDWDLGEQT...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q197F8</td>\n",
       "      <td>002R_IIV3</td>\n",
       "      <td>Uncharacterized protein 002R OS=Invertebrate i...</td>\n",
       "      <td>MASNTVSAQGGSNRPVRDFSNIQDVAQFLLFDPIWNEQPGSIVPWK...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q197F7</td>\n",
       "      <td>003L_IIV3</td>\n",
       "      <td>Uncharacterized protein 003L OS=Invertebrate i...</td>\n",
       "      <td>MYQAINPCPQSWYGSPQLEREIVCKMSGAPHYPNYYPVHPNALGGA...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q6GZX2</td>\n",
       "      <td>003R_FRG3G</td>\n",
       "      <td>Uncharacterized protein 3R OS=Frog virus 3 (is...</td>\n",
       "      <td>MARPLLGKTSSVRRRLESLSACSIFFFLRKFCQKMASLVFLNSPVY...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570152</th>\n",
       "      <td>Q6UY62</td>\n",
       "      <td>Z_SABVB</td>\n",
       "      <td>RING finger protein Z OS=Sabia mammarenavirus ...</td>\n",
       "      <td>MGNSKSKSKLSANQYEQQTVNSTKQVAILKRQAEPSLYGRHNCRCC...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570153</th>\n",
       "      <td>P08105</td>\n",
       "      <td>Z_SHEEP</td>\n",
       "      <td>Putative uncharacterized protein Z OS=Ovis ari...</td>\n",
       "      <td>MSSSLEITSFYSFIWTPHIGPLLFGIGLWFSMFKEPSHFCPCQHPH...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570154</th>\n",
       "      <td>Q88470</td>\n",
       "      <td>Z_TACVF</td>\n",
       "      <td>RING finger protein Z OS=Tacaribe virus (strai...</td>\n",
       "      <td>MGNCNRTQKPSSSSNNLEKPPQAAEFRRTAEPSLYGRYNCKCCWFA...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570155</th>\n",
       "      <td>A9JR22</td>\n",
       "      <td>Z_TAMVU</td>\n",
       "      <td>RING finger protein Z OS=Tamiami mammarenaviru...</td>\n",
       "      <td>MGLRYSKEVRDRHGDKDPEGRIPITQTMPQTLYGRYNCKSCWFANK...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570156</th>\n",
       "      <td>B2ZDY1</td>\n",
       "      <td>Z_WWAVU</td>\n",
       "      <td>RING finger protein Z OS=Whitewater Arroyo mam...</td>\n",
       "      <td>MGLRYSKDVKDRYGDREPEGRIPITLNMPQSLYGRYNCKSCWFANK...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569192 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       accession        name  \\\n",
       "0         Q6GZX4  001R_FRG3G   \n",
       "1         Q6GZX3  002L_FRG3G   \n",
       "2         Q197F8   002R_IIV3   \n",
       "3         Q197F7   003L_IIV3   \n",
       "4         Q6GZX2  003R_FRG3G   \n",
       "...          ...         ...   \n",
       "570152    Q6UY62     Z_SABVB   \n",
       "570153    P08105     Z_SHEEP   \n",
       "570154    Q88470     Z_TACVF   \n",
       "570155    A9JR22     Z_TAMVU   \n",
       "570156    B2ZDY1     Z_WWAVU   \n",
       "\n",
       "                                              description  \\\n",
       "0       Putative transcription factor 001R OS=Frog vir...   \n",
       "1       Uncharacterized protein 002L OS=Frog virus 3 (...   \n",
       "2       Uncharacterized protein 002R OS=Invertebrate i...   \n",
       "3       Uncharacterized protein 003L OS=Invertebrate i...   \n",
       "4       Uncharacterized protein 3R OS=Frog virus 3 (is...   \n",
       "...                                                   ...   \n",
       "570152  RING finger protein Z OS=Sabia mammarenavirus ...   \n",
       "570153  Putative uncharacterized protein Z OS=Ovis ari...   \n",
       "570154  RING finger protein Z OS=Tacaribe virus (strai...   \n",
       "570155  RING finger protein Z OS=Tamiami mammarenaviru...   \n",
       "570156  RING finger protein Z OS=Whitewater Arroyo mam...   \n",
       "\n",
       "                                                 sequence embedding  \n",
       "0       MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQV...        []  \n",
       "1       MSIIGATRLQNDKSDTYSAGPCYAGGCSAFTPRGTCGKDWDLGEQT...        []  \n",
       "2       MASNTVSAQGGSNRPVRDFSNIQDVAQFLLFDPIWNEQPGSIVPWK...        []  \n",
       "3       MYQAINPCPQSWYGSPQLEREIVCKMSGAPHYPNYYPVHPNALGGA...        []  \n",
       "4       MARPLLGKTSSVRRRLESLSACSIFFFLRKFCQKMASLVFLNSPVY...        []  \n",
       "...                                                   ...       ...  \n",
       "570152  MGNSKSKSKLSANQYEQQTVNSTKQVAILKRQAEPSLYGRHNCRCC...        []  \n",
       "570153  MSSSLEITSFYSFIWTPHIGPLLFGIGLWFSMFKEPSHFCPCQHPH...        []  \n",
       "570154  MGNCNRTQKPSSSSNNLEKPPQAAEFRRTAEPSLYGRYNCKCCWFA...        []  \n",
       "570155  MGLRYSKEVRDRHGDKDPEGRIPITQTMPQTLYGRYNCKSCWFANK...        []  \n",
       "570156  MGLRYSKDVKDRYGDREPEGRIPITLNMPQSLYGRYNCKSCWFANK...        []  \n",
       "\n",
       "[569192 rows x 5 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "df[\"embedding\"] = df[\"sequence\"].apply(lambda x: np.zeros(0))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3983/562736 [07:14<16:56:00,  9.17it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m j \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m tqdm(not_computed_yet\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(not_computed_yet)):\n\u001b[0;32m----> 5\u001b[0m     embed \u001b[38;5;241m=\u001b[39m \u001b[43membed_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msequence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malphabet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     df\u001b[38;5;241m.\u001b[39mat[i, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m embed\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# save to disk every once and a while\u001b[39;00m\n",
      "File \u001b[0;32m~/protein-embed/notebooks/protein_clip.py:81\u001b[0m, in \u001b[0;36membed_sequence\u001b[0;34m(sequence, model, alphabet, embed_layer, device)\u001b[0m\n\u001b[1;32m     79\u001b[0m _, _, tokens \u001b[38;5;241m=\u001b[39m batch_converter([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, sequence\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<mask>\u001b[39m\u001b[38;5;124m\"\u001b[39m))])\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 81\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepr_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43membed_layer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_contacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m token_representations \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepresentations\u001b[39m\u001b[38;5;124m\"\u001b[39m][embed_layer]\n\u001b[1;32m     85\u001b[0m rep \u001b[38;5;241m=\u001b[39m token_representations[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m : \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(sequence)]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/esm/model/esm2.py:112\u001b[0m, in \u001b[0;36mESM2.forward\u001b[0;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[0m\n\u001b[1;32m    109\u001b[0m     padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m--> 112\u001b[0m     x, attn \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_head_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (layer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m repr_layers:\n\u001b[1;32m    118\u001b[0m         hidden_representations[layer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/esm/modules.py:138\u001b[0m, in \u001b[0;36mTransformerLayer.forward\u001b[0;34m(self, x, self_attn_mask, self_attn_padding_mask, need_head_weights)\u001b[0m\n\u001b[1;32m    136\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    137\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm(x)\n\u001b[0;32m--> 138\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mgelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/esm/modules.py:24\u001b[0m, in \u001b[0;36mgelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgelu\u001b[39m(x):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implementation of the gelu activation function.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    For information: OpenAI GPT's gelu is slightly different\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    (and gives slightly different results):\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "checkpoint_every = 1_000\n",
    "not_computed_yet = df[df[\"embedding\"].apply(len) == 0]\n",
    "j = 0\n",
    "for i, row in tqdm(not_computed_yet.iterrows(), total=len(not_computed_yet)):\n",
    "    embed = embed_sequence(row[\"sequence\"], model, alphabet, layer)\n",
    "    df.at[i, \"embedding\"] = embed\n",
    "    \n",
    "    # save to disk every once and a while\n",
    "    if j > checkpoint_every:\n",
    "        df.to_parquet(f\"uniprot-esm2-layers{layer}.parquet\")\n",
    "        j = 0\n",
    "    j += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
